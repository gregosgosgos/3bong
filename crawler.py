# crawler.py
import os, re, json, asyncio
import pandas as pd
from urllib.parse import urlparse, parse_qs, urlencode, urlunparse, urljoin
from dotenv import load_dotenv
from playwright.async_api import async_playwright, TimeoutError as PwTimeoutError
import gspread
from google.oauth2.service_account import Credentials

# ======================
# í™˜ê²½ì„¤ì • / ê²€ì¦
# ======================
load_dotenv()
SITE_BASE = os.getenv("SITE_BASE", "https://3bong.kr").rstrip("/")
LOGIN_URL = os.getenv("LOGIN_URL", f"{SITE_BASE}/member/login.php")
USER_ID   = os.getenv("USER_ID")
USER_PW   = os.getenv("USER_PW")
SHEET_ID  = os.getenv("SHEET_ID")
SHEET_TAB = os.getenv("SHEET_TAB", "í¬ë¡¤ë§ê²°ê³¼")

# ì„±ëŠ¥/ì¬ê³  ê´€ë ¨ ì˜µì…˜(.env)
ENABLE_STOCK = os.getenv("ENABLE_STOCK", "1") == "1"       # 0ì´ë©´ ì¬ê³  ìˆ˜ì§‘ ìŠ¤í‚µ
STOCK_MODE = os.getenv("STOCK_MODE", "http")               # "http" (ë¹ ë¦„) ë˜ëŠ” "dialog" (ì •í™•ë„â†‘)
STOCK_CONCURRENCY = int(os.getenv("STOCK_CONCURRENCY", "20"))  # ë™ì‹œ ìš”ì²­ ìˆ˜(HTTP ëª¨ë“œ)
STOCK_TIMEOUT_MS  = int(os.getenv("STOCK_TIMEOUT_MS", "2000"))  # ê° HTTP ìš”ì²­ íƒ€ì„ì•„ì›ƒ(ms)
TREAT_SILENT_AS_ZERO = os.getenv("TREAT_SILENT_AS_ZERO", "1") == "1"  # dialog ëª¨ë“œì—ì„œ ë¬´ë°˜ì‘+10000 ìœ ì§€ì‹œ 0 ê¸°ë¡
MAX_PAGES = int(os.getenv("MAX_PAGES", "0") or "0")        # í…ŒìŠ¤íŠ¸ìš© í˜ì´ì§€ ì œí•œ(0=ë¬´ì œí•œ)

required_keys = ["USER_ID", "USER_PW", "SHEET_ID"]
missing = [k for k in required_keys if not os.getenv(k)]
if missing:
    raise RuntimeError(f"í™˜ê²½ë³€ìˆ˜ ëˆ„ë½: {', '.join(missing)}. "
                       "ë¡œì»¬ì€ .env, GitHub ActionsëŠ” Secretsë¡œ ì„¤ì •í•˜ì„¸ìš”.")

# ======================
# ì¹´í…Œê³ ë¦¬
# ======================
CATE_CODES = {
    "ê³¼ì/ì¿ í‚¤/ìŠ¤ë‚µ": "021013",
    "ì´ˆì½œë¦¿ë¥˜": "021005",
    "ìº”ë””(ì‚¬íƒ•)/ì¹´ë¼ë©œ": "021002",
    "ì ¤ë¦¬/ê»Œ/ê°€ë£¨ì¿¡": "021015",
    "ê±´ê²¬ê³¼/ì–´í¬/ìœ¡í¬": "021004",
    "ì¤‘êµ­ê°„ì‹": "021012",
    "ìŒë£Œ/í‘¸ë”©": "021003",
}

# ======================
# ì…€ë ‰í„°
# ======================
CARD_SEL       = "div.item_cont"
NAME_SEL       = ".item_name"
PRICE_FALLBACK = ".item_price span"
DETAIL_LINKSEL = "a[href*='goods_view']"
CODE_ATTR_SEL  = "[data-goods-no]"
PHOTO_BOX_SEL  = ".item_photo_box"

# ======================
# ìœ í‹¸
# ======================
def clean_price_text(txt: str | None) -> int | None:
    if not txt: return None
    m = re.sub(r"[^\d]", "", txt)
    return int(m) if m else None

def strip_brand_prefix(name: str) -> str:
    """
    ë§¨ ì• 'ë¸Œëœë“œëª…)' íŒ¨í„´ë§Œ ì œê±° (ì¤‘ê°„ ê´„í˜¸ëŠ” ìœ ì§€)
    ì˜ˆ) 'ì˜¤ì„±) ë ˆëª¬ë§› ìƒŒë“œ 227g' -> 'ë ˆëª¬ë§› ìƒŒë“œ 227g'
        'ì‚¼ì§„)ë°”ì‚­í”„ë ˆì²¼ë²„í„°ê°ˆë¦­ë§› 45g' -> 'ë°”ì‚­í”„ë ˆì²¼ë²„í„°ê°ˆë¦­ë§› 45g'
    """
    if not name: return name
    m = re.match(r'^\s*([^\(\)\[\]]{1,30})\)\s*(.+)$', name)
    return m.group(2).strip() if m else name

def parse_name_pack_expiry(raw_name: str | None):
    """
    ë°˜í™˜: (ìƒí’ˆëª…, ë¬¶ìŒí˜•íƒœ("íƒ€/ë°•/ê°œ"), ë¬¶ìŒë‹¹ìˆ˜ëŸ‰(int), ìœ í†µê¸°í•œ("YY.MM.DD"))
    """
    if not raw_name:
        return None, None, None, None
    lines = [re.sub(r"\s+", " ", l.strip()) for l in raw_name.splitlines() if l.strip()]
    base_name = lines[0] if lines else raw_name.strip()
    base_name = strip_brand_prefix(base_name)
    tail_text = " ".join(lines[1:]) if len(lines) > 1 else ""

    unit = None; qty = None; expiry = None
    for m in re.finditer(r"\(([^)]*)\)", tail_text):
        inside = m.group(1)
        mu = re.search(r"(íƒ€|ë°•|ê°œ)", inside)
        if mu: unit = mu.group(1)
        mq = re.search(r"(\d+)\s*ê°œì…", inside)
        if mq:
            try: qty = int(mq.group(1))
            except: qty = None
        if unit or (qty is not None):
            break
    md = re.search(r"(\d{2}\.\d{2}\.\d{2})", tail_text)
    if md: expiry = md.group(1)
    return base_name, unit, qty, expiry

def build_list_url(cate_code: str, page: int) -> str:
    base = f"{SITE_BASE}/goods/goods_list.php"
    parsed = urlparse(base)
    qs = {"cateCd": cate_code, "page": str(page)}
    return urlunparse(parsed._replace(query=urlencode(qs)))

def absolutize_img(url: str | None) -> str | None:
    # ì´ë¯¸ì§€ ìª½ì€ ê¸°ì¡´ ë¡œì§ ìœ ì§€(ì´ë¯¸ ì •ìƒ ë™ì‘)
    if not url: return None
    if url.startswith("http"): return url
    if url.startswith("//"):   return "https:" + url
    if url.startswith("/"):    return f"{SITE_BASE}{url}"
    return f"{SITE_BASE}/goods/{url.lstrip('./')}"

async def text_of(el):
    if not el: return None
    try:
        t = await el.inner_text()
        if t: return t
    except: pass
    try:
        return await el.text_content()
    except:
        return None

# ======================
# ë¡œê·¸ì¸
# ======================
async def login(page):
    ID_BOX = ["input[name='m_id']", "#loginId", "input[name='loginId']"]
    PW_BOX = ["input[name='m_pwd']", "#loginPwd", "input[type='password']"]
    SUBMIT = ["button[type='submit']", "input[type='submit']", "#btnLogin"]

    await page.goto(LOGIN_URL, wait_until="domcontentloaded")
    id_el = pw_el = None
    for s in ID_BOX:
        try: id_el = await page.wait_for_selector(s, timeout=2000); break
        except: pass
    for s in PW_BOX:
        try: pw_el = await page.wait_for_selector(s, timeout=2000); break
        except: pass
    if not (id_el and pw_el):
        raise RuntimeError("ë¡œê·¸ì¸ ì…ë ¥ì¹¸ì„ ëª» ì°¾ì•˜ìŠµë‹ˆë‹¤.")
    await id_el.fill(USER_ID); await pw_el.fill(USER_PW)
    clicked = False
    for s in SUBMIT:
        btn = await page.query_selector(s)
        if btn: await btn.click(); clicked = True; break
    if not clicked: await page.keyboard.press("Enter")
    await page.wait_for_load_state("networkidle")

# ======================
# í˜ì´ì§€ ìˆ˜ ì¶”ì •
# ======================
async def get_max_page_on(page) -> int:
    last = await page.query_selector('a[aria-label="Last"], a.last, a[href*="page="]:has-text("ë")')
    if last:
        href = await last.get_attribute("href") or ""
        m = re.search(r"[?&]page=(\d+)", href)
        if m:
            try: return int(m.group(1))
            except: pass
    links = await page.query_selector_all('a[href*="page="]')
    mx = 1
    for a in links[:25]:
        h = await a.get_attribute("href") or ""
        m = re.search(r"[?&]page=(\d+)", h)
        if m:
            try:
                v = int(m.group(1))
                if v > mx: mx = v
            except: pass
    return mx

# ======================
# í’ˆì ˆ ì œì™¸
# ======================
async def is_soldout(card) -> bool:
    try:
        li_has = await card.evaluate("(el) => el.closest('li')?.classList?.contains('item_soldout') ?? false")
        if li_has: return True
    except: pass
    if await card.query_selector("strong.item_soldout_bg"):
        return True
    return False

# ======================
# ì¸ë„¤ì¼
# ======================
async def extract_thumbnail(card) -> str | None:
    box = await card.query_selector(PHOTO_BOX_SEL)
    if not box:
        img = await card.query_selector("img[data-original], img[src]")
        if img:
            for attr in ("data-original", "src"):
                v = await img.get_attribute(attr)
                if v: return absolutize_img(v)
        return None
    for attr in ("data-image-list", "data-image-main", "data-image-detail"):
        v = await box.get_attribute(attr)
        if v: return absolutize_img(v)
    img = await box.query_selector("img[data-original], img[src]")
    if img:
        for attr in ("data-original", "src"):
            v = await img.get_attribute(attr)
            if v: return absolutize_img(v)
    return None

# ======================
# íŒë§¤ê°€/íŒë§¤ìˆ˜ëŸ‰ ì„ íƒ (ë§ˆì§„ 10~20%)
# ======================
SALE_CANDIDATES = [1900, 2900, 3900]
FEE_RATE = 0.22
BAND_MIN = 0.10
BAND_MAX = 0.20

def choose_sale_combo(unit_cost: float | None):
    if unit_cost is None or unit_cost <= 0:
        return None, None, None
    feasible = []
    for sp in SALE_CANDIDATES:
        net = sp * (1 - FEE_RATE)
        if net <= 0: continue
        q_max = int((1 - BAND_MIN) * net // unit_cost)  # floor(0.9*net/unit_cost)
        if q_max >= 1:
            margin = 1 - (q_max * unit_cost) / net
            feasible.append((sp, q_max, margin))
    if not feasible:
        return None, None, None
    in_band = [r for r in feasible if BAND_MIN <= r[2] <= BAND_MAX]
    if in_band:
        in_band.sort(key=lambda x: (-x[2], x[0]))    # ë§ˆì§„ ë‚´ë¦¼ì°¨ìˆœ, ë™ë¥ ì´ë©´ ë‚®ì€ íŒë§¤ê°€
        return in_band[0]
    above = [r for r in feasible if r[2] > BAND_MAX]
    if above:
        above.sort(key=lambda x: (x[2] - BAND_MAX, x[0]))  # ì´ˆê³¼í­ ìµœì†Œ, ë™ë¥ ì´ë©´ ë‚®ì€ íŒë§¤ê°€
        return above[0]
    return None, None, None

# ======================
# ì¹´ë“œ â†’ ê¸°ì´ˆ í–‰ (ì¬ê³  ì œì™¸)
# ======================
async def parse_card_to_row_base(page, card, cate_name: str, current_url: str):
    if await is_soldout(card):
        return None

    raw_name = await text_of(await card.query_selector(NAME_SEL))
    prod_name, pack_unit, pack_qty, expiry = parse_name_pack_expiry(raw_name)

    bundle_price = None
    holder = await card.query_selector("[data-goods-price]")
    if holder:
        raw = await holder.get_attribute("data-goods-price")
        if raw:
            try: bundle_price = int(round(float(raw)))
            except: bundle_price = None
    if bundle_price is None:
        txt = await text_of(await card.query_selector(PRICE_FALLBACK))
        bundle_price = clean_price_text(txt)

    # ìƒì„¸ ë§í¬(ìƒì„¸ë§Œ urljoinìœ¼ë¡œ ì •í™•íˆ)
    full = None; code = None
    link = await card.query_selector(DETAIL_LINKSEL)
    if link:
        href = await link.get_attribute("href")
        if href:
            full = urljoin(current_url, href)   # âœ… í•µì‹¬: goods/goods/goods_view.php ë°©ì§€
            qs = parse_qs(urlparse(full).query)
            code = qs.get("goodsNo", [None])[0]
    if not code:
        holder2 = await card.query_selector(CODE_ATTR_SEL)
        if holder2:
            code = await holder2.get_attribute("data-goods-no")

    thumb = await extract_thumbnail(card)

    unit_cost = None
    if bundle_price is not None and pack_qty:
        unit_cost = round(bundle_price / pack_qty)

    sell_price, sell_qty, margin = choose_sale_combo(unit_cost)

    if not prod_name:
        return None

    return {
        "ì¹´í…Œê³ ë¦¬": cate_name,
        "ìƒí’ˆëª…": prod_name,
        "ìœ í†µê¸°í•œ": expiry,
        "ë¬¶ìŒë‹¹ìˆ˜ëŸ‰": pack_qty,
        "ë¬¶ìŒë‹¨ê°€": bundle_price,
        "ê°œë‹¹ë‹¨ê°€": unit_cost,
        "ì¬ê³ ìˆ˜ëŸ‰": None,          # <- ì´í›„ ë³´ê°•
        "íŒë§¤ìˆ˜ëŸ‰": sell_qty,
        "íŒë§¤ê°€": sell_price,
        "ë§ˆì§„ìœ¨": round(margin, 4) if margin is not None else None,
        "URL": full,
        "ì´ë¯¸ì§€URL": thumb,
    }

# ======================
# ì¹´í…Œê³ ë¦¬ í¬ë¡¤
# ======================
async def crawl_category(page, cate_name: str, cate_code: str):
    rows = []
    first = build_list_url(cate_code, 1)
    await page.goto(first, wait_until="domcontentloaded")
    max_page = await get_max_page_on(page)
    if max_page < 1: max_page = 1
    if MAX_PAGES and max_page > MAX_PAGES:
        max_page = MAX_PAGES
    print(f"[{cate_name}] ìµœëŒ€ {max_page}í˜ì´ì§€ ì¶”ì •")

    for p in range(1, max_page + 1):
        url = build_list_url(cate_code, p)
        await page.goto(url, wait_until="domcontentloaded")
        cards = await page.query_selector_all(CARD_SEL)
        print(f"[{cate_name}] {p}í˜ì´ì§€ ì¹´ë“œìˆ˜: {len(cards)}")
        if not cards:
            break
        for c in cards:
            row = await parse_card_to_row_base(page, c, cate_name, url)
            if row: rows.append(row)
    return rows

# ======================
# ì¬ê³ ìˆ˜ëŸ‰ (HTTP ëª¨ë“œ: ë¹ ë¦„)
# ======================
async def fetch_stock_http(request_ctx, url: str) -> int | None:
    # ì§§ì€ íƒ€ì„ì•„ì›ƒ + 1íšŒ ì¬ì‹œë„
    async def _once():
        try:
            resp = await request_ctx.get(url, timeout=STOCK_TIMEOUT_MS)
            if not resp.ok:
                return None
            html = await resp.text()
            stocks = [int(s.replace(",", "")) for s in re.findall(r'data-stock\s*=\s*["\']?([\d,]+)', html)]
            return max(stocks) if stocks else None
        except:
            return None
    v = await _once()
    if v is not None:
        return v
    return await _once()

# ======================
# ì¬ê³ ìˆ˜ëŸ‰ (dialog ëª¨ë“œ: ìƒì„¸ ì—´ì–´ 10000 ì…ë ¥â†’ê²½ê³  íŒŒì‹±, ì •í™•ë„â†‘)
# ======================
async def fetch_stock_from_detail(page, goods_url: str) -> int | None:
    """
    ìš°ì„ ìˆœìœ„:
    1) DOMì˜ input[data-stock] (ìµœìš°ì„ )
    2) ìˆ˜ëŸ‰ì¹¸ì— 10000 ì…ë ¥ â†’ dialog/í˜ì´ì§€ í…ìŠ¤íŠ¸: "ìµœëŒ€ìˆ˜ëŸ‰ì€ ####ì´í•˜"
    3) ì´ë²¤íŠ¸ í›„ ë‹¤ì‹œ data-stock ì¬í™•ì¸
    4) (ì˜µì…˜) ë¬´ë°˜ì‘+10000 ìœ ì§€ë©´ 0
    """
    tmp = await page.context.new_page()
    try:
        tmp.set_default_timeout(9000)
        await tmp.goto(goods_url, wait_until="domcontentloaded", timeout=9000)

        async def read_dom_stocks():
            try:
                vals = await tmp.evaluate("""
                    () => Array.from(document.querySelectorAll('input[data-stock]'))
                               .map(el => el.getAttribute('data-stock'))
                """)
                found = []
                for v in vals or []:
                    if v:
                        n = re.sub(r"[^\d]", "", v)
                        if n.isdigit():
                            found.append(int(n))
                return max(found) if found else None
            except:
                return None

        # 1) ë¨¼ì € data-stock ë°”ë¡œ ì½ê¸°
        v0 = await read_dom_stocks()
        if v0 is not None:
            return v0

        # ìˆ˜ëŸ‰ input í›„ë³´
        selectors = [
            "input[name='goodsCnt[]']","input[name='goodsCnt']","input[name^='goodsCnt']",
            "[id*='option_display'] input[type='text']",".goods_qty input[type='text']",
            "input.text.goodsCnt_0","input.text","input[type='text']"
        ]
        qty = None
        for sel in selectors:
            try:
                await tmp.wait_for_selector(sel, timeout=2500)
                els = await tmp.query_selector_all(sel)
                if not els: continue
                ranked = []
                for el in els:
                    name = (await el.get_attribute("name")) or ""
                    ds   = await el.get_attribute("data-stock")
                    rank = (1 if "goodsCnt" in name else 0) + (1 if ds else 0)
                    ranked.append((rank, el))
                ranked.sort(key=lambda x: -x[0])
                qty = ranked[0][1]
                break
            except PwTimeoutError:
                continue
        if not qty:
            return None

        # dialog ìº¡ì²˜
        dialog_msg = {"text": None}
        def _on_dialog(d):
            dialog_msg["text"] = d.message
            asyncio.create_task(d.dismiss())
        tmp.on("dialog", _on_dialog)

        # 10000 ì…ë ¥ + ì´ë²¤íŠ¸ íŠ¸ë¦¬ê±°
        await qty.click()
        await qty.fill("")
        await qty.type("10000", delay=5)
        await tmp.evaluate("""
            (el) => {
                el.dispatchEvent(new Event('input', {bubbles:true}));
                el.dispatchEvent(new Event('change',{bubbles:true}));
            }
        """, qty)
        await tmp.evaluate("el => el.blur()", qty)
        try: await qty.press("Enter")
        except: pass
        await tmp.wait_for_timeout(1200)

        # 2) dialog ìš°ì„ 
        if dialog_msg["text"]:
            m = re.search(r"ìµœëŒ€ìˆ˜ëŸ‰ì€\s*([\d,]+)\s*ì´í•˜", dialog_msg["text"])
            if m: return int(m.group(1).replace(",", ""))

        # 2-ë³´ì¡°) í˜ì´ì§€ ë‚´ í…ìŠ¤íŠ¸
        for sel in ["text=ìµœëŒ€ìˆ˜ëŸ‰ì€","div:has-text('ìµœëŒ€ìˆ˜ëŸ‰ì€')",
                    "[class*='alert']:has-text('ìµœëŒ€ìˆ˜ëŸ‰ì€')",
                    "#option_display_item_0 :has-text('ìµœëŒ€ìˆ˜ëŸ‰ì€')"]:
            try:
                el = await tmp.wait_for_selector(sel, timeout=1200)
                if el:
                    t = await el.text_content() or ""
                    m = re.search(r"ìµœëŒ€ìˆ˜ëŸ‰ì€\s*([\d,]+)\s*ì´í•˜", t)
                    if m: return int(m.group(1).replace(",", ""))
            except PwTimeoutError:
                continue

        # 3) ì´ë²¤íŠ¸ í›„ ë‹¤ì‹œ data-stock ì¬í™•ì¸
        v1 = await read_dom_stocks()
        if v1 is not None:
            return v1

        # 4) ì™„ì „ ë¬´ë°˜ì‘ & ê°’ì´ 10000ì´ë©´ (ì˜µì…˜) 0
        try:
            val = await qty.input_value()
            if val and re.sub(r"[^\d]", "", val) == "10000":
                return 0 if TREAT_SILENT_AS_ZERO else None
        except:
            pass
        return None
    finally:
        await tmp.close()

# ======================
# ì¬ê³  ë³‘ë ¬ ë³´ê°• (ëª¨ë“œ ìŠ¤ìœ„ì¹˜ ì§€ì›)
# ======================
async def enrich_stocks_concurrently(context, rows: list[dict]):
    if not ENABLE_STOCK:
        return rows

    targets = [(i, r["URL"]) for i, r in enumerate(rows) if r.get("URL")]
    if not targets:
        return rows

    # dialog ëª¨ë“œëŠ” ìƒì„¸ íƒ­ì„ ì—´ì–´ì•¼ í•´ì„œ ê³¼ë„í•œ ë³‘ë ¬ì€ ë¹„ì¶”ì²œ(4~6 ì •ë„ ê¶Œì¥)
    concurrency = STOCK_CONCURRENCY if STOCK_MODE == "http" else min(6, max(1, STOCK_CONCURRENCY))
    sem = asyncio.Semaphore(max(1, concurrency))

    async def worker_http(idx: int, url: str):
        async with sem:
            v = await fetch_stock_http(context.request, url)
            return idx, v

    async def worker_dialog(idx: int, url: str):
        async with sem:
            # context.pages[0]ëŠ” ë¡œê·¸ì¸ëœ ë©”ì¸ í˜ì´ì§€. ìƒì„¸ íƒ­ì€ í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ ì—´ê³  ë‹«ìŒ.
            page0 = context.pages[0]
            v = await fetch_stock_from_detail(page0, url)
            return idx, v

    tasks = [
        asyncio.create_task(
            worker_http(i, u) if STOCK_MODE == "http" else worker_dialog(i, u)
        ) for i, u in targets
    ]

    for fut in asyncio.as_completed(tasks):
        idx, stock = await fut
        rows[idx]["ì¬ê³ ìˆ˜ëŸ‰"] = stock
    return rows

# ======================
# Google Sheets
# ======================
def get_gspread_client():
    scopes = [
        "https://www.googleapis.com/auth/spreadsheets",
        "https://www.googleapis.com/auth/drive",
    ]
    json_str = os.getenv("GOOGLE_SERVICE_ACCOUNT_JSON")
    file_path = os.getenv("GOOGLE_SERVICE_ACCOUNT_FILE")
    if json_str:
        info = json.loads(json_str)
        creds = Credentials.from_service_account_info(info, scopes=scopes)
    elif file_path:
        creds = Credentials.from_service_account_file(file_path, scopes=scopes)
    else:
        raise RuntimeError("ì„œë¹„ìŠ¤ ê³„ì • í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. GOOGLE_SERVICE_ACCOUNT_JSON ë˜ëŠ” GOOGLE_SERVICE_ACCOUNT_FILEì„ ì„¤ì •í•˜ì„¸ìš”.")
    return gspread.authorize(creds)

def upload_df_to_sheet(df: pd.DataFrame, sheet_id: str, tab_name: str):
    gc = get_gspread_client()
    sh = gc.open_by_key(sheet_id)
    try:
        ws = sh.worksheet(tab_name)
    except gspread.exceptions.WorksheetNotFound:
        ws = sh.add_worksheet(title=tab_name, rows="100", cols="20")

    cols = ["ì¹´í…Œê³ ë¦¬","ìƒí’ˆëª…","ìœ í†µê¸°í•œ","ë¬¶ìŒë‹¹ìˆ˜ëŸ‰","ë¬¶ìŒë‹¨ê°€","ê°œë‹¹ë‹¨ê°€",
            "ì¬ê³ ìˆ˜ëŸ‰","íŒë§¤ìˆ˜ëŸ‰","íŒë§¤ê°€","ë§ˆì§„ìœ¨","URL","ì´ë¯¸ì§€URL"]
    df = df.reindex(columns=cols)

    ws.clear()
    values = [cols] + df.astype(object).where(pd.notna(df), "").values.tolist()
    ws.update(range_name="A1", values=values, value_input_option="USER_ENTERED")

# ======================
# ë©”ì¸
# ======================
async def main():
    print("ğŸš€ crawler start")
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        ctx = await browser.new_context(
            locale="ko-KR",
            timezone_id="Asia/Seoul",
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36"
        )

        # ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ ë¹¨ë¦¬: ì´ë¯¸ì§€/ë¯¸ë””ì–´/í°íŠ¸ ì°¨ë‹¨
        async def route_intercept(route):
            rt = route.request.resource_type
            if rt in {"image", "media", "font"}:
                return await route.abort()
            return await route.continue_()
        await ctx.route("**/*", route_intercept)
        ctx.set_default_timeout(6000)

        page = await ctx.new_page()
        print("ğŸ”‘ try login...")
        await login(page)
        print("âœ… login ok")

        # 1) ëª©ë¡ ìˆ˜ì§‘ (ì¬ê³  ì œì™¸)
        all_rows = []
        for name, code in CATE_CODES.items():
            items = await crawl_category(page, name, code)
            print(f"[ì™„ë£Œ] {name}({code}) -> {len(items)}ê°œ")
            all_rows += items

        # 2) ì¬ê³  ë³´ê°•
        if ENABLE_STOCK:
            mode = "HTTP" if STOCK_MODE == "http" else "DIALOG"
            print(f"ğŸ” ì¬ê³  ìˆ˜ì§‘ ì‹œì‘ (ëª¨ë“œ {mode}, ë™ì‹œ {STOCK_CONCURRENCY})...")
            all_rows = await enrich_stocks_concurrently(ctx, all_rows)

        df = pd.DataFrame(all_rows).drop_duplicates().reset_index(drop=True)
        print(f"ì´ í–‰ìˆ˜: {len(df)}")

        # ë¡œì»¬ ë°±ì—…
        df.to_csv("3bong_products.csv", index=False, encoding="utf-8-sig")

        # ì‹œíŠ¸ ì—…ë¡œë“œ
        upload_df_to_sheet(df, SHEET_ID, SHEET_TAB)
        print(f"âœ… êµ¬ê¸€ ì‹œíŠ¸ ì—…ë¡œë“œ ì™„ë£Œ: sheet={SHEET_ID}, tab={SHEET_TAB}")

        await ctx.close(); await browser.close()

if __name__ == "__main__":
    asyncio.run(main())
